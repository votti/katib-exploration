{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp.v2.dsl import (\n",
    "    component,\n",
    "    Input,\n",
    "    Output,\n",
    "    Dataset,\n",
    "    Metrics,\n",
    ")\n",
    "from kfp.v2 import dsl\n",
    "from kfp import components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = kfp.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_data_op = components.load_component_from_url(\n",
    "    \"https://raw.githubusercontent.com/kubeflow/pipelines/master/components/contrib/web/Download/component-sdk-v2.yaml\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_mnist_op = components.load_component_from_text(\n",
    "    \"\"\"\n",
    "name: Parse MNIST\n",
    "inputs:\n",
    "- {name: Images, description: gziped images in the idx format}\n",
    "- {name: Labels, description: gziped labels in the idx format}\n",
    "outputs:\n",
    "- {name: Dataset}\n",
    "metadata:\n",
    "  annotations:\n",
    "    author: Vito Zanotelli\n",
    "    description: Based on https://github.com/kubeflow/pipelines/blob/master/components/contrib/sample/Python_script/component.yaml\n",
    "implementation:\n",
    "  container:\n",
    "    image: tensorflow/tensorflow:2.7.1\n",
    "    command:\n",
    "    - sh\n",
    "    - -ec\n",
    "    - |\n",
    "      # This is how additional packages can be installed dynamically\n",
    "      python3 -m pip install pip idx2numpy\n",
    "      # Run the rest of the command after installing the packages.\n",
    "      \"$0\" \"$@\"\n",
    "    - python3\n",
    "    - -u  # Auto-flush. We want the logs to appear in the console immediately.\n",
    "    - -c  # Inline scripts are easy, but have size limitaions and the error traces do not show source lines.\n",
    "    - |\n",
    "      import gzip\n",
    "      import idx2numpy\n",
    "      import sys\n",
    "      from pathlib import Path\n",
    "      import pickle\n",
    "      import tensorflow as tf\n",
    "      img_path = sys.argv[1]\n",
    "      label_path = sys.argv[2]\n",
    "      output_path = sys.argv[3]\n",
    "      with gzip.open(img_path, 'rb') as f:\n",
    "        x = idx2numpy.convert_from_string(f.read())\n",
    "      with gzip.open(label_path, 'rb') as f:\n",
    "        y = idx2numpy.convert_from_string(f.read())\n",
    "      #one-hot encode the categories\n",
    "      x_out = tf.convert_to_tensor(x)\n",
    "      y_out = tf.keras.utils.to_categorical(y)\n",
    "      Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "      with open(output_path, 'wb') as output_file:\n",
    "            pickle.dump((x_out, y_out), output_file)\n",
    "    - {inputPath: Images}\n",
    "    - {inputPath: Labels}\n",
    "    - {outputPath: Dataset}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"tensorflow/tensorflow:2.7.1\",  # Optional\n",
    "    packages_to_install=[\"scikit-learn\"],  # Optional\n",
    ")\n",
    "def process(\n",
    "    data_raw: Input[Dataset],\n",
    "    data_processed: Output[Dataset],\n",
    "    val_pct: float = 0.2,\n",
    "    trainset_flag: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Here we do all the preprocessing\n",
    "    if the data path is for training data we:\n",
    "    (1) Normalize the data\n",
    "    (2) split the train and val data\n",
    "    If it is for unseen test data, we:\n",
    "    (1) Normalize the data\n",
    "    This function returns in any case the processed data path\n",
    "    \"\"\"\n",
    "    # sklearn\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import pickle\n",
    "    import numpy as np\n",
    "\n",
    "    def img_norm(x):\n",
    "        return np.reshape(x / 255, list(x.shape) + [1])\n",
    "\n",
    "    with open(data_raw.path, \"rb\") as f:\n",
    "        x, y = pickle.load(f)\n",
    "    if trainset_flag:\n",
    "\n",
    "        x_ = img_norm(x)\n",
    "        x_train, x_val, y_train, y_val = train_test_split(\n",
    "            x_, y, test_size=val_pct, stratify=y, random_state=42\n",
    "        )\n",
    "\n",
    "        with open(data_processed.path, \"wb\") as output_file:\n",
    "            pickle.dump((x_train, y_train, x_val, y_val), output_file)\n",
    "\n",
    "    else:\n",
    "        x_ = img_norm(x)\n",
    "        with open(data_processed.path, \"wb\") as output_file:\n",
    "            pickle.dump((x_, y), output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(base_image=\"tensorflow/tensorflow:2.7.1\", packages_to_install=[\"scipy\"])\n",
    "def train(\n",
    "    data_train: Input[Dataset],\n",
    "    model_out: Output[Dataset],\n",
    "    metrics: Output[Metrics],\n",
    "    lr: float = 1e-4,\n",
    "    optimizer: str = \"Adam\",\n",
    "    loss: str = \"categorical_crossentropy\",\n",
    "    epochs: int = 1,\n",
    "    batch_size: int = 32,\n",
    "):\n",
    "    \"\"\"\n",
    "    This is the simulated train part of our ML pipeline where training is performed\n",
    "    \"\"\"\n",
    "\n",
    "    import tensorflow as tf\n",
    "    import pickle\n",
    "    from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "    with open(data_train.path, \"rb\") as f:\n",
    "        x_train, y_train, x_val, y_val = pickle.load(f)\n",
    "\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.Conv2D(\n",
    "                64, (3, 3), activation=\"relu\", input_shape=(28, 28, 1)\n",
    "            ),\n",
    "            tf.keras.layers.MaxPooling2D(2, 2),\n",
    "            tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "            tf.keras.layers.MaxPooling2D(2, 2),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(10, activation=\"softmax\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    if optimizer.lower() == \"sgd\":\n",
    "        optimizer = tf.keras.optimizers.SGD(lr)\n",
    "    else:\n",
    "        optimizer = tf.keras.optimizers.Adam(lr)\n",
    "\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "    # fit the model\n",
    "    model_early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_accuracy\", patience=10, verbose=1, restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    train_datagen = ImageDataGenerator(horizontal_flip=False)\n",
    "\n",
    "    validation_datagen = ImageDataGenerator()\n",
    "    history = model.fit(\n",
    "        train_datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_datagen.flow(x_val, y_val, batch_size=batch_size),\n",
    "        shuffle=False,\n",
    "        callbacks=[model_early_stopping_callback],\n",
    "    )\n",
    "\n",
    "    model.save(model_out.path, save_format=\"tf\")\n",
    "    # Log accuracz\n",
    "    print(history.history[\"accuracy\"])\n",
    "    print(history.history[\"val_accuracy\"])\n",
    "\n",
    "    metrics.log_metric(\"accuracy\", history.history[\"accuracy\"][-1])\n",
    "    metrics.log_metric(\"val-accuracy\", history.history[\"val_accuracy\"][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=\"Download MNIST dataset\",\n",
    "    description=\"A pipeline to download the MNIST dataset files\",\n",
    ")\n",
    "def mnist_training_pipeline(\n",
    "    lr: float = 1e-4,\n",
    "    optimizer: str = \"Adam\",\n",
    "    loss: str = \"categorical_crossentropy\",\n",
    "    epochs: int = 3,\n",
    "    batch_size: int = 5,\n",
    "):\n",
    "    TRAIN_IMG_URL = \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\"\n",
    "    TRAIN_LAB_URL = \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\"\n",
    "\n",
    "    train_imgs = download_data_op(TRAIN_IMG_URL)\n",
    "    train_imgs.set_display_name(\"Download training images\")\n",
    "    train_y = download_data_op(TRAIN_LAB_URL)\n",
    "    train_y.set_display_name(\"Download training labels\")\n",
    "\n",
    "    mnist_train = parse_mnist_op(train_imgs.output, train_y.output)\n",
    "    mnist_train.set_display_name(\"Prepare train dataset\")\n",
    "\n",
    "    processed_train = process(mnist_train.output, val_pct=0.2, trainset_flag=True)\n",
    "    processed_train.set_display_name(\"Preprocess images\")\n",
    "\n",
    "    model_out = train(\n",
    "        processed_train.output,\n",
    "        lr=lr,\n",
    "        optimizer=optimizer,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        loss=loss,\n",
    "    )\n",
    "    model_out.set_display_name(\"Fit the model\")\n",
    "    return mnist_train.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = client.create_run_from_pipeline_func(\n",
    "    mnist_training_pipeline,\n",
    "    mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE,\n",
    "    # You can optionally override your pipeline_root when submitting the run too:\n",
    "    # pipeline_root='gs://my-pipeline-root/example-pipeline',\n",
    "    arguments={},\n",
    "    experiment_name=\"mnist\",\n",
    "    run_name=\"training_mnist_classifier\",\n",
    "    namespace=\"vito-zanotelli\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a parameter tuning with Katib - not working for v2 yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "from typing import List\n",
    "\n",
    "import kfp.dsl as dsl1\n",
    "\n",
    "from kubernetes.client.models import V1ObjectMeta\n",
    "from kubeflow.katib import ApiClient\n",
    "from kubeflow.katib import KatibClient\n",
    "from kubeflow.katib import V1beta1Experiment\n",
    "from kubeflow.katib import V1beta1ExperimentSpec\n",
    "from kubeflow.katib import V1beta1AlgorithmSpec\n",
    "from kubeflow.katib import V1beta1ObjectiveSpec\n",
    "from kubeflow.katib import V1beta1ParameterSpec\n",
    "from kubeflow.katib import V1beta1FeasibleSpace\n",
    "from kubeflow.katib import V1beta1TrialTemplate\n",
    "from kubeflow.katib import V1beta1TrialParameterSpec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to build a katib experiment, we require a trial spec.\n",
    "\n",
    "In this case the trial spec is an Argo workflow produced form the Kubeflow pipeline.\n",
    "\n",
    "The requirement to run this Argo workflow, the integration needs to be setup:\n",
    "https://github.com/kubeflow/katib/tree/master/examples/v1beta1/argo/README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trial_spec(pipeline, params_list: List[dsl1.PipelineParam]):\n",
    "    \"\"\"\n",
    "    Create a Katib trial specification from a KFP pipeline function\n",
    "\n",
    "    Args:\n",
    "        pipeline: a kubeflow pipeline function\n",
    "        params_list (List[dsl.PipelineParam]): a list of pipeline parameters. These need\n",
    "            to map the pipeline parameter to the Katib parameter.\n",
    "            Eg: [dsl.PipelineParam(name='lr', value='${trialParameters.learningRate}')]\n",
    "\n",
    "    \"\"\"\n",
    "    compiler = kfp.compiler.Compiler(mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE)\n",
    "    return compiler._create_workflow(pipeline, params_list=params_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trial_parameters():\n",
    "    \"\"\"\n",
    "    Defines the search space for trial parameters\n",
    "    \"\"\"\n",
    "    # Experiment search space.\n",
    "    # In this example we tune learning rate and batch size.\n",
    "\n",
    "    parameters = [\n",
    "        V1beta1ParameterSpec(\n",
    "            name=\"learning_rate\",\n",
    "            parameter_type=\"double\",\n",
    "            feasible_space=V1beta1FeasibleSpace(min=\"0.00001\", max=\"0.001\"),\n",
    "        ),\n",
    "    ]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trial_template(trial_spec):\n",
    "\n",
    "    trial_template = V1beta1TrialTemplate(\n",
    "        primary_container_name=\"main\",  # Name of the primary container returning the metrics in the workflow\n",
    "        primary_pod_labels={\"katib.kubeflow.org/model-training\": \"true\"},\n",
    "        trial_parameters=[\n",
    "            V1beta1TrialParameterSpec(\n",
    "                name=\"learningRate\",  # the parameter name that is replaced in your template (see Trial Specification).\n",
    "                description=\"Learning rate for the training model\",\n",
    "                reference=\"learning_rate\",  # the parameter name that experiment’s suggestion returns (parameter name in the Parameters Specification).\n",
    "            )\n",
    "        ],\n",
    "        trial_spec=trial_spec,\n",
    "        success_condition='status.[@this].#(phase==\"Succeeded\")#',\n",
    "        failure_condition='status.[@this].#(phase==\"Failed\")#',\n",
    "        retain=True,  # Retain completed pods - left hear for easier debugging\n",
    "    )\n",
    "    return trial_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_katib_experiment_spec(\n",
    "    trial_spec,\n",
    "    max_trial_count: int = 2,\n",
    "    max_failed_trial_count: int = 1,\n",
    "    parallel_trial_count: int = 2,\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates the full Katib experiment\n",
    "\n",
    "    Args:\n",
    "        trial_spec: the trial specification\n",
    "        max_trial_count (int): max number of trials to run, default: 2\n",
    "        max_failed_trial_count (int): max number of failed trials before stopping, default: 1\n",
    "        parallel_trial_count (int): max trials to run in parallel, default: 2\n",
    "\n",
    "    Returns:\n",
    "        A Katib experiment specification\n",
    "    \"\"\"\n",
    "\n",
    "    # Objective specification.\n",
    "    objective = V1beta1ObjectiveSpec(\n",
    "        type=\"maximize\",\n",
    "        goal=0.9,\n",
    "        objective_metric_name=\"val-accuracy\",\n",
    "        additional_metric_names=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    # Algorithm specification, see docu https://www.kubeflow.org/docs/components/katib/experiment/#search-algorithms-in-detail\n",
    "    algorithm = V1beta1AlgorithmSpec(\n",
    "        algorithm_name=\"random\",\n",
    "    )\n",
    "\n",
    "    parameters = create_trial_parameters()\n",
    "\n",
    "    # trial_spec = create_trial_spec(training_steps)\n",
    "\n",
    "    # Configure parameters for the Trial template.\n",
    "    trial_template = create_trial_template(trial_spec)\n",
    "\n",
    "    # Create an Experiment from the above parameters.\n",
    "    experiment_spec = V1beta1ExperimentSpec(\n",
    "        # Experimental Budget\n",
    "        max_trial_count=max_trial_count,\n",
    "        max_failed_trial_count=max_failed_trial_count,\n",
    "        parallel_trial_count=parallel_trial_count,\n",
    "        # Optimization Objective\n",
    "        objective=objective,\n",
    "        # Optimization Algorithm\n",
    "        algorithm=algorithm,\n",
    "        # Optimization Parameters\n",
    "        parameters=parameters,\n",
    "        # Trial Template\n",
    "        trial_template=trial_template,\n",
    "    )\n",
    "\n",
    "    return experiment_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare actual specs for current run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_spec = create_trial_spec(\n",
    "    mnist_training_pipeline,\n",
    "    params_list=[dsl.PipelineParam(name=\"lr\", value=\"${trialParameters.learningRate}\")],\n",
    ")\n",
    "\n",
    "# Somehow the pipeline is configured with the wrong serviceAccountName by default\n",
    "trial_spec[\"spec\"][\"serviceAccountName\"] = \"default-editor\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(mnist_training_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 ('katib-exp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "346a4e9d8b8e6802b68a0916b92683cfb1882082eeafaaae0a3525ab995e1047"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
