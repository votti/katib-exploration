{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import kfp.components as components\n",
    "import kfp.dsl as dsl\n",
    "from kfp.components import InputPath, OutputPath, create_component_from_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = kfp.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_data_op = components.load_component_from_url(\n",
    "    \"https://raw.githubusercontent.com/kubeflow/pipelines/master/components/contrib/web/Download/component.yaml\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_mnist_op = components.load_component_from_text(\n",
    "    \"\"\"\n",
    "name: Parse MNIST\n",
    "inputs:\n",
    "- {name: Images, description: gziped images in the idx format}\n",
    "- {name: Labels, description: gziped labels in the idx format}\n",
    "outputs:\n",
    "- {name: Dataset}\n",
    "metadata:\n",
    "  annotations:\n",
    "    author: Vito Zanotelli\n",
    "    description: Based on https://github.com/kubeflow/pipelines/blob/master/components/contrib/sample/Python_script/component.yaml\n",
    "implementation:\n",
    "  container:\n",
    "    image: tensorflow/tensorflow:2.7.1\n",
    "    command:\n",
    "    - sh\n",
    "    - -ec\n",
    "    - |\n",
    "      # This is how additional packages can be installed dynamically\n",
    "      python3 -m pip install pip idx2numpy\n",
    "      # Run the rest of the command after installing the packages.\n",
    "      \"$0\" \"$@\"\n",
    "    - python3\n",
    "    - -u  # Auto-flush. We want the logs to appear in the console immediately.\n",
    "    - -c  # Inline scripts are easy, but have size limitaions and the error traces do not show source lines.\n",
    "    - |\n",
    "      import gzip\n",
    "      import idx2numpy\n",
    "      import sys\n",
    "      from pathlib import Path\n",
    "      import pickle\n",
    "      import tensorflow as tf\n",
    "      img_path = sys.argv[1]\n",
    "      label_path = sys.argv[2]\n",
    "      output_path = sys.argv[3]\n",
    "      with gzip.open(img_path, 'rb') as f:\n",
    "        x = idx2numpy.convert_from_string(f.read())\n",
    "      with gzip.open(label_path, 'rb') as f:\n",
    "        y = idx2numpy.convert_from_string(f.read())\n",
    "      #one-hot encode the categories\n",
    "      x_out = tf.convert_to_tensor(x)\n",
    "      y_out = tf.keras.utils.to_categorical(y)\n",
    "      Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "      with open(output_path, 'wb') as output_file:\n",
    "            pickle.dump((x_out, y_out), output_file)\n",
    "    - {inputPath: Images}\n",
    "    - {inputPath: Labels}\n",
    "    - {outputPath: Dataset}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(\n",
    "    data_raw_path: InputPath(str),  # type: ignore\n",
    "    data_processed_path: OutputPath(str),  # type: ignore\n",
    "    val_pct: float = 0.2,\n",
    "    trainset_flag: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Here we do all the preprocessing\n",
    "    if the data path is for training data we:\n",
    "    (1) Normalize the data\n",
    "    (2) split the train and val data\n",
    "    If it is for unseen test data, we:\n",
    "    (1) Normalize the data\n",
    "    This function returns in any case the processed data path\n",
    "    \"\"\"\n",
    "    # sklearn\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import pickle\n",
    "    import numpy as np\n",
    "\n",
    "    def img_norm(x):\n",
    "        return np.reshape(x / 255, list(x.shape) + [1])\n",
    "\n",
    "    with open(data_raw_path, \"rb\") as f:\n",
    "        x, y = pickle.load(f)\n",
    "    if trainset_flag:\n",
    "\n",
    "        x_ = img_norm(x)\n",
    "        x_train, x_val, y_train, y_val = train_test_split(\n",
    "            x_, y, test_size=val_pct, stratify=y, random_state=42\n",
    "        )\n",
    "\n",
    "        with open(data_processed_path, \"wb\") as output_file:\n",
    "            pickle.dump((x_train, y_train, x_val, y_val), output_file)\n",
    "\n",
    "    else:\n",
    "        x_ = img_norm(x)\n",
    "        with open(data_processed_path, \"wb\") as output_file:\n",
    "            pickle.dump((x_, y), output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_op = create_component_from_func(\n",
    "    func=process,\n",
    "    base_image=\"tensorflow/tensorflow:2.7.1\",  # Optional\n",
    "    packages_to_install=[\"scikit-learn\"],  # Optional\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    data_train_path: InputPath(str),  # type: ignore\n",
    "    model_out_path: OutputPath(str),  # type: ignore\n",
    "    mlpipeline_metrics_path: OutputPath(\"Metrics\"),  # type: ignore # noqa: F821\n",
    "    metrics_log_path: OutputPath(str),  # type: ignore\n",
    "    lr: float = 1e-4,\n",
    "    optimizer: str = \"Adam\",\n",
    "    loss: str = \"categorical_crossentropy\",\n",
    "    epochs: int = 1,\n",
    "    batch_size: int = 32,\n",
    "):\n",
    "    \"\"\"\n",
    "    This is the simulated train part of our ML pipeline where training is performed\n",
    "    \"\"\"\n",
    "\n",
    "    import tensorflow as tf\n",
    "    import pickle\n",
    "    from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "    import json\n",
    "\n",
    "    with open(data_train_path, \"rb\") as f:\n",
    "        x_train, y_train, x_val, y_val = pickle.load(f)\n",
    "\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.Conv2D(\n",
    "                64, (3, 3), activation=\"relu\", input_shape=(28, 28, 1)\n",
    "            ),\n",
    "            tf.keras.layers.MaxPooling2D(2, 2),\n",
    "            tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "            tf.keras.layers.MaxPooling2D(2, 2),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(10, activation=\"softmax\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    if optimizer.lower() == \"sgd\":\n",
    "        optimizer = tf.keras.optimizers.SGD(lr)\n",
    "    else:\n",
    "        optimizer = tf.keras.optimizers.Adam(lr)\n",
    "\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "    # fit the model\n",
    "    model_early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_accuracy\", patience=10, verbose=1, restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    train_datagen = ImageDataGenerator(horizontal_flip=False)\n",
    "\n",
    "    validation_datagen = ImageDataGenerator()\n",
    "    history = model.fit(\n",
    "        train_datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_datagen.flow(x_val, y_val, batch_size=batch_size),\n",
    "        shuffle=False,\n",
    "        callbacks=[model_early_stopping_callback],\n",
    "    )\n",
    "\n",
    "    model.save(model_out_path, save_format=\"tf\")\n",
    "    # Log accuracz\n",
    "    print(history.history[\"accuracy\"])\n",
    "    print(history.history[\"val_accuracy\"])\n",
    "\n",
    "    metrics = {\n",
    "        \"metrics\": [\n",
    "            {\n",
    "                \"name\": \"accuracy\",  # The name of the metric. Visualized as the column name in the runs table.\n",
    "                \"numberValue\": history.history[\"accuracy\"][\n",
    "                    -1\n",
    "                ],  # The value of the metric. Must be a numeric value.\n",
    "                \"format\": \"PERCENTAGE\",  # The optional format of the metric. Supported values are \"RAW\" (displayed in raw format) and \"PERCENTAGE\" (displayed in percentage format).\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"val-accuracy\",  # The name of the metric. Visualized as the column name in the runs table.\n",
    "                \"numberValue\": history.history[\"val_accuracy\"][\n",
    "                    -1\n",
    "                ],  # The value of the metric. Must be a numeric value.\n",
    "                \"format\": \"PERCENTAGE\",  # The optional format of the metric. Supported values are \"RAW\" (displayed in raw format) and \"PERCENTAGE\" (displayed in percentage format).\n",
    "            },\n",
    "        ]\n",
    "    }\n",
    "    with open(mlpipeline_metrics_path, \"w\") as f:\n",
    "        json.dump(metrics, f)\n",
    "\n",
    "    # Output metrics for Katib\n",
    "    with open(metrics_log_path, \"w\") as f:\n",
    "        f.write(f\"val-accuracy={history.history['val_accuracy'][0]}\\n\")\n",
    "        f.write(f\"accuracy={history.history['accuracy'][0]}\\n\")\n",
    "    print(metrics_log_path)\n",
    "\n",
    "\n",
    "train_op = create_component_from_func(\n",
    "    func=train, base_image=\"tensorflow/tensorflow:2.7.1\", packages_to_install=[\"scipy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _label_cache(step):\n",
    "    \"\"\"Helper to add pod cache label\n",
    "\n",
    "    Somehow in our configuration the wrong cache label is applied :/\n",
    "    \"\"\"\n",
    "    step.add_pod_label(\"pipelines.kubeflow.org/cache_enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=\"Download MNIST dataset\",\n",
    "    description=\"A pipeline to download the MNIST dataset files\",\n",
    ")\n",
    "def mnist_training_pipeline(\n",
    "    lr: float = 1e-4,\n",
    "    optimizer: str = \"Adam\",\n",
    "    loss: str = \"categorical_crossentropy\",\n",
    "    epochs: int = 3,\n",
    "    batch_size: int = 5,\n",
    "):\n",
    "    TRAIN_IMG_URL = \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\"\n",
    "    TRAIN_LAB_URL = \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\"\n",
    "\n",
    "    train_imgs = download_data_op(TRAIN_IMG_URL)\n",
    "    train_imgs.set_display_name(\"Download training images\")\n",
    "    _label_cache(train_imgs)\n",
    "\n",
    "    train_y = download_data_op(TRAIN_LAB_URL)\n",
    "    train_y.set_display_name(\"Download training labels\")\n",
    "    _label_cache(train_y)\n",
    "\n",
    "    mnist_train = parse_mnist_op(train_imgs.output, train_y.output)\n",
    "    mnist_train.set_display_name(\"Prepare train dataset\")\n",
    "\n",
    "    processed_train = process_op(mnist_train.output, val_pct=0.2, trainset_flag=True)\n",
    "    processed_train.set_display_name(\"Preprocess images\")\n",
    "\n",
    "    training_output = (\n",
    "        train_op(\n",
    "            processed_train.outputs[\"data_processed\"],\n",
    "            lr=lr,\n",
    "            optimizer=optimizer,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            loss=loss,\n",
    "        )\n",
    "        .set_cpu_limit(\"1\")\n",
    "        .set_memory_limit(\"1Gi\")\n",
    "    )\n",
    "    training_output.set_display_name(\"Fit the model\")\n",
    "    return mnist_train.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = client.create_run_from_pipeline_func(\n",
    "    mnist_training_pipeline,\n",
    "    mode=kfp.dsl.PipelineExecutionMode.V1_LEGACY,\n",
    "    # You can optionally override your pipeline_root when submitting the run too:\n",
    "    # pipeline_root='gs://my-pipeline-root/example-pipeline',\n",
    "    arguments={},\n",
    "    experiment_name=\"mnist\",\n",
    "    run_name=\"training_mnist_classifier_13\",\n",
    "    namespace=\"vito-zanotelli\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter tuning with Katib\n",
    "\n",
    "We now want to do parameter tuning over the pipeline with Katib.\n",
    "\n",
    "This requires:\n",
    "- adding a label to the step from which parameters should be collected\n",
    "- preventing that the step generating parameters is not skipped due to caching\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_training_pipeline_katib(\n",
    "    lr: float = 1e-4,\n",
    "    optimizer: str = \"Adam\",\n",
    "    loss: str = \"categorical_crossentropy\",\n",
    "    epochs: int = 1,\n",
    "    batch_size: int = 32,\n",
    "):\n",
    "\n",
    "    TRAIN_IMG_URL = \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\"\n",
    "    TRAIN_LAB_URL = \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\"\n",
    "\n",
    "    train_imgs = download_data_op(TRAIN_IMG_URL)\n",
    "    train_imgs.set_display_name(\"Download training images\")\n",
    "    _label_cache(train_imgs)\n",
    "\n",
    "    train_y = download_data_op(TRAIN_LAB_URL)\n",
    "    train_y.set_display_name(\"Download training labels\")\n",
    "    _label_cache(train_y)\n",
    "\n",
    "    mnist_train = parse_mnist_op(train_imgs.output, train_y.output)\n",
    "    mnist_train.set_display_name(\"Prepare train dataset\")\n",
    "\n",
    "    processed_train = process_op(mnist_train.output, val_pct=0.2, trainset_flag=True)\n",
    "    processed_train.set_display_name(\"Preprocess images\")\n",
    "\n",
    "    training_output = (\n",
    "        train_op(\n",
    "            processed_train.outputs[\"data_processed\"],\n",
    "            lr=lr,\n",
    "            optimizer=optimizer,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            loss=loss,\n",
    "        )\n",
    "        .set_cpu_limit(\"1\")\n",
    "        .set_memory_limit(\"1Gi\")\n",
    "    )\n",
    "\n",
    "    training_output.set_display_name(\"Fit the model\")\n",
    "    # This step needs to run always, as otherwise the metrics cannot be collected.\n",
    "    # Other steps are cached if appropriate\n",
    "    training_output.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "\n",
    "    # This pod label indicates which pod Katib should collect the metric from.\n",
    "    # A metrics collecting sidecar container will be added\n",
    "    training_output.add_pod_label(\"katib.kubeflow.org/model-training\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = client.create_run_from_pipeline_func(\n",
    "    mnist_training_pipeline_katib,\n",
    "    mode=kfp.dsl.PipelineExecutionMode.V1_LEGACY,\n",
    "    # You can optionally override your pipeline_root when submitting the run too:\n",
    "    # pipeline_root='gs://my-pipeline-root/example-pipeline',\n",
    "    arguments={},\n",
    "    experiment_name=\"mnist\",\n",
    "    run_name=\"training_mnist_classifier_katib\",\n",
    "    namespace=\"vito-zanotelli\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now setup katib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from typing import List\n",
    "\n",
    "import kfp.dsl as dsl\n",
    "from kfp import components\n",
    "\n",
    "\n",
    "from kubernetes.client.models import V1ObjectMeta\n",
    "from kubeflow.katib import ApiClient\n",
    "from kubeflow.katib import KatibClient\n",
    "from kubeflow.katib import V1beta1Experiment\n",
    "from kubeflow.katib import V1beta1ExperimentSpec\n",
    "from kubeflow.katib import V1beta1AlgorithmSpec\n",
    "from kubeflow.katib import V1beta1ObjectiveSpec\n",
    "from kubeflow.katib import V1beta1ParameterSpec\n",
    "from kubeflow.katib import V1beta1FeasibleSpace\n",
    "from kubeflow.katib import V1beta1TrialTemplate\n",
    "from kubeflow.katib import V1beta1TrialParameterSpec"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to build a katib experiment, we require a trial spec.\n",
    "\n",
    "In this case the trial spec is an Argo workflow produced form the Kubeflow pipeline.\n",
    "\n",
    "The requirement to run this Argo workflow, the integration needs to be setup.\n",
    "\n",
    "\n",
    "### Setup of Katib Argo workflow integration\n",
    "If you are running on a full Kubeflow installation *DO NOT INSTALL ARGO* as this will likely break your installation.\n",
    "\n",
    "Just run the following commands:\n",
    "\n",
    "Enable side-car injection:\n",
    "\n",
    "`kubectl patch namespace argo -p '{\"metadata\":{\"labels\":{\"katib.kubeflow.org/metrics-collector-injection\":\"enabled\"}}}'`\n",
    "\n",
    "\n",
    "Verify that the emissary executor is active (should be default in newer Kubeflow installations):\n",
    "\n",
    "` kubectl get ConfigMap -n argo workflow-controller-configmap -o yaml | grep containerRuntimeExecutor`\n",
    "\n",
    "Patch the Katib controller:\n",
    "\n",
    "`kubectl patch ClusterRole katib-controller -n kubeflow --type=json \\\n",
    "  -p='[{\"op\": \"add\", \"path\": \"/rules/-\", \"value\": {\"apiGroups\":[\"argoproj.io\"],\"resources\":[\"workflows\"],\"verbs\":[\"get\", \"list\", \"watch\", \"create\", \"delete\"]}}]'\n",
    "`\n",
    "\n",
    "`kubectl patch Deployment katib-controller -n kubeflow --type=json \\\n",
    "  -p='[{\"op\": \"add\", \"path\": \"/spec/template/spec/containers/0/args/-\", \"value\": \"--trial-resources=Workflow.v1alpha1.argoproj.io\"}]'`\n",
    "\n",
    "For more details and how to set this up on a partial Kubeflow installation follow:\n",
    "https://github.com/kubeflow/katib/tree/master/examples/v1beta1/argo/README.md"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions to build the individual Katib Experiment Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trial_spec(pipeline, params_list: List[dsl.PipelineParam]):\n",
    "    \"\"\"\n",
    "    Create a Katib trial specification from a KFP pipeline function\n",
    "\n",
    "    Args:\n",
    "        pipeline: a kubeflow pipeline function\n",
    "        params_list (List[dsl.PipelineParam]): a list of pipeline parameters. These need\n",
    "            to map the pipeline parameter to the Katib parameter.\n",
    "            Eg: [dsl.PipelineParam(name='lr', value='${trialParameters.learningRate}')]\n",
    "\n",
    "    \"\"\"\n",
    "    compiler = kfp.compiler.Compiler(mode=kfp.dsl.PipelineExecutionMode.V1_LEGACY)\n",
    "    return compiler._create_workflow(pipeline, params_list=params_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trial_parameters():\n",
    "    \"\"\"\n",
    "    Defines the search space for trial parameters\n",
    "    \"\"\"\n",
    "    # Experiment search space.\n",
    "    # In this example we tune learning rate and batch size.\n",
    "\n",
    "    parameters = [\n",
    "        V1beta1ParameterSpec(\n",
    "            name=\"learning_rate\",\n",
    "            parameter_type=\"double\",\n",
    "            feasible_space=V1beta1FeasibleSpace(min=\"0.00001\", max=\"0.001\"),\n",
    "        ),\n",
    "    ]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trial_template(trial_spec):\n",
    "\n",
    "    trial_template = V1beta1TrialTemplate(\n",
    "        primary_container_name=\"main\",  # Name of the primary container returning the metrics in the workflow\n",
    "        primary_pod_labels={\"katib.kubeflow.org/model-training\": \"true\"},\n",
    "        trial_parameters=[\n",
    "            V1beta1TrialParameterSpec(\n",
    "                name=\"learningRate\",  # the parameter name that is replaced in your template (see Trial Specification).\n",
    "                description=\"Learning rate for the training model\",\n",
    "                reference=\"learning_rate\",  # the parameter name that experimentâ€™s suggestion returns (parameter name in the Parameters Specification).\n",
    "            )\n",
    "        ],\n",
    "        trial_spec=trial_spec,\n",
    "        success_condition='status.[@this].#(phase==\"Succeeded\")#',\n",
    "        failure_condition='status.[@this].#(phase==\"Failed\")#',\n",
    "        retain=True,  # Retain completed pods - left hear for easier debugging\n",
    "    )\n",
    "    return trial_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metrics_collector_spec():\n",
    "    \"\"\"This defines the custom metrics collector\"\"\"\n",
    "    return {\n",
    "        \"source\": {\n",
    "            \"fileSystemPath\": {\n",
    "                \"path\": \"/tmp/outputs/mlpipeline_metrics/data\",\n",
    "                \"kind\": \"File\",\n",
    "            }\n",
    "        },\n",
    "        \"collector\": {\n",
    "            \"kind\": \"Custom\",\n",
    "            \"customCollector\": {\n",
    "                \"args\": [\n",
    "                    \"-m\",\n",
    "                    \"val-accuracy;accuracy\",\n",
    "                    \"-s\",\n",
    "                    \"katib-db-manager.kubeflow:6789\",\n",
    "                    \"-t\",\n",
    "                    \"$(PodName)\",\n",
    "                    \"-path\",\n",
    "                    \"/tmp/outputs/mlpipeline_metrics\",\n",
    "                ],\n",
    "                \"image\": \"votti/kfpv1-metricscollector:v0.0.10\",\n",
    "                \"imagePullPolicy\": \"Always\",\n",
    "                \"name\": \"custom-metrics-logger-and-collector\",\n",
    "                \"env\": [\n",
    "                    {\n",
    "                        \"name\": \"PodName\",\n",
    "                        \"valueFrom\": {\"fieldRef\": {\"fieldPath\": \"metadata.name\"}},\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "        },\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_katib_experiment_spec(\n",
    "    trial_spec,\n",
    "    max_trial_count: int = 2,\n",
    "    max_failed_trial_count: int = 1,\n",
    "    parallel_trial_count: int = 2,\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates the full Katib experiment\n",
    "\n",
    "    Args:\n",
    "        trial_spec: the trial specification\n",
    "        max_trial_count (int): max number of trials to run, default: 2\n",
    "        max_failed_trial_count (int): max number of failed trials before stopping, default: 1\n",
    "        parallel_trial_count (int): max trials to run in parallel, default: 2\n",
    "\n",
    "    Returns:\n",
    "        A Katib experiment specification\n",
    "    \"\"\"\n",
    "\n",
    "    # Objective specification.\n",
    "    objective = V1beta1ObjectiveSpec(\n",
    "        type=\"maximize\",\n",
    "        goal=0.9,\n",
    "        objective_metric_name=\"val-accuracy\",\n",
    "        additional_metric_names=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    # Algorithm specification, see docu https://www.kubeflow.org/docs/components/katib/experiment/#search-algorithms-in-detail\n",
    "    algorithm = V1beta1AlgorithmSpec(\n",
    "        algorithm_name=\"random\",\n",
    "    )\n",
    "\n",
    "    parameters = create_trial_parameters()\n",
    "\n",
    "    # trial_spec = create_trial_spec(training_steps)\n",
    "\n",
    "    # Configure parameters for the Trial template.\n",
    "    trial_template = create_trial_template(trial_spec)\n",
    "\n",
    "    # Metrics collector spec\n",
    "    metrics_collector = create_metrics_collector_spec()\n",
    "\n",
    "    # Create an Experiment from the above parameters.\n",
    "    experiment_spec = V1beta1ExperimentSpec(\n",
    "        # Experimental Budget\n",
    "        max_trial_count=max_trial_count,\n",
    "        max_failed_trial_count=max_failed_trial_count,\n",
    "        parallel_trial_count=parallel_trial_count,\n",
    "        # Optimization Objective\n",
    "        objective=objective,\n",
    "        # Optimization Algorithm\n",
    "        algorithm=algorithm,\n",
    "        # Optimization Parameters\n",
    "        parameters=parameters,\n",
    "        # Trial Template\n",
    "        trial_template=trial_template,\n",
    "        # Metrics collector\n",
    "        metrics_collector_spec=metrics_collector,\n",
    "    )\n",
    "\n",
    "    return experiment_spec"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Katib Experiment from components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_spec = create_trial_spec(\n",
    "    mnist_training_pipeline_katib,\n",
    "    params_list=[dsl.PipelineParam(name=\"lr\", value=\"${trialParameters.learningRate}\")],\n",
    ")\n",
    "\n",
    "# Somehow the pipeline is configured with the wrong serviceAccountName by default\n",
    "trial_spec[\"spec\"][\"serviceAccountName\"] = \"default-editor\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "katib_spec = create_katib_experiment_spec(trial_spec)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to generate a full experiment the api_version, kind and namespace need to be defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "katib_experiment = V1beta1Experiment(\n",
    "    api_version=\"kubeflow.org/v1beta1\",\n",
    "    kind=\"Experiment\",\n",
    "    metadata=V1ObjectMeta(\n",
    "        name=\"katib-kfp-mnist-custom-31\",\n",
    "        namespace=\"vito-zanotelli\",\n",
    "    ),\n",
    "    spec=katib_spec,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated yaml can written out to submit via the web ui:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"experiment_template_kfp_mnist_4.yaml\", \"w\") as f:\n",
    "    yaml.dump(ApiClient().sanitize_for_serialization(katib_experiment), f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or sumitted via the KatibClient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = KatibClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.create_experiment(katib_experiment)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now be able to observe in the Web UI how the Katib\n",
    "Experiment is running.\n",
    "\n",
    "To see how the `Argo Workflows` are started, you can also check the Kubernetes cluster:\n",
    "\n",
    "`kubectl get Workflow -n <namespace>`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 ('katib-exp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "346a4e9d8b8e6802b68a0916b92683cfb1882082eeafaaae0a3525ab995e1047"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
